{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3692f7d5-f0a4-4227-8743-6c9cf96f8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156faf93-0571-43c7-842c-01baa809f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 20:41:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "Spark master: local[*]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-health-check\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark master:\", spark.sparkContext.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "449913a3-54d9-46a2-b118-18bf4fbee801",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DeltaTable.forName(spark, \"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22abfe3f-eb47-4e98-9963-cfa630064380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b67964-798e-4c20-8e58-f4d6de04a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "dt.update(\n",
    "    condition= \"id='1'\",\n",
    "    set = {\n",
    "        \"name\": f\"'gowda'\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351a427e-1498-4be2-ae10-e063031a7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "|_c0|  _c1|_c2|\n",
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "|  1| prem| 28|\n",
      "|  2|kumar| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"./test.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f16e34-bfdb-411e-816d-ff9653ae0953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 20:18:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"./test.csv\").write.format('delta').save(\"/data/managed/test2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7465e65-5686-4934-ba14-dcdf9bcd5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "|_c0|  _c1|_c2|\n",
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "|  1| prem| 28|\n",
      "|  2|kumar| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.load(\"/data/managed/test2/\", format=\"delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037fc749-89a8-41b3-8ae1-eb2fe69835a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 20:43:35 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`default`.`users` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "26/01/27 20:43:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "26/01/27 20:43:36 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "26/01/27 20:43:36 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "26/01/27 20:43:36 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"header\", True).option(\"overwriteSchema\", \"true\").csv(\"./test.csv\").write.format(\"delta\").mode('overwrite').saveAsTable(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e5dae6-dff6-40ba-b8fd-6e2e84565378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 20:43:16 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0f8bee9-2732-4ccc-800d-194b9f46addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|gowda| 28|\n",
      "|  2|kumar| 28|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"users\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
